{"instruction": "Write code to load a pretrained language model using transformers", "input": "", "output": "```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\n# Load model and tokenizer\nmodel_name = \"mistralai/Mistral-7B-v0.1\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n)\n\n# Set pad token if not present\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\nprint(f\"Model loaded: {model.config._name_or_path}\")\nprint(f\"Model size: {model.num_parameters():,} parameters\")\n```"}
{"instruction": "Show how to set up QLoRA training with 4-bit quantization", "input": "", "output": "```python\nfrom transformers import BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\n# Configure 4-bit quantization\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\n# Load model with quantization\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)\n\n# Prepare for k-bit training\nmodel = prepare_model_for_kbit_training(model)\n\n# Configure LoRA\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\n# Get PEFT model\nmodel = get_peft_model(model, peft_config)\nmodel.print_trainable_parameters()\n```"}
{"instruction": "Write a function to load and preprocess a dataset for instruction tuning", "input": "Dataset format: JSONL with 'instruction', 'input', 'output' fields", "output": "```python\nfrom datasets import Dataset\nimport json\n\ndef load_instruction_dataset(file_path: str, tokenizer, max_length: int = 512):\n    \"\"\"Load and preprocess instruction tuning dataset\"\"\"\n\n    # Load data\n    with open(file_path, 'r') as f:\n        data = [json.loads(line) for line in f]\n\n    # Format prompts\n    def format_prompt(example):\n        if example.get('input', ''):\n            prompt = f\"### Instruction:\\n{example['instruction']}\\n\\n### Input:\\n{example['input']}\\n\\n### Response:\\n{example['output']}\"\n        else:\n            prompt = f\"### Instruction:\\n{example['instruction']}\\n\\n### Response:\\n{example['output']}\"\n        return prompt\n\n    # Tokenize function\n    def tokenize_function(examples):\n        prompts = [format_prompt(ex) for ex in examples]\n        model_inputs = tokenizer(\n            prompts,\n            max_length=max_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n        model_inputs[\"labels\"] = model_inputs[\"input_ids\"].clone()\n        return model_inputs\n\n    # Create dataset\n    dataset = Dataset.from_list(data)\n    tokenized_dataset = dataset.map(\n        lambda x: tokenize_function([x]),\n        batched=False,\n        remove_columns=dataset.column_names\n    )\n\n    return tokenized_dataset\n\n# Usage\ndataset = load_instruction_dataset(\"train.jsonl\", tokenizer)\nprint(f\"Dataset size: {len(dataset)}\")\n```"}
{"instruction": "Create a complete training script for QLoRA fine-tuning", "input": "", "output": "```python\nfrom transformers import TrainingArguments, Trainer\nimport torch\nfrom datetime import datetime\n\ndef train_qlora_model(model, tokenizer, train_dataset, output_dir):\n    \"\"\"Train model with QLoRA\"\"\"\n\n    # Training arguments\n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        num_train_epochs=3,\n        per_device_train_batch_size=4,\n        gradient_accumulation_steps=4,\n        gradient_checkpointing=True,\n        optim=\"paged_adamw_32bit\",\n        logging_steps=10,\n        save_strategy=\"steps\",\n        save_steps=100,\n        learning_rate=2e-4,\n        weight_decay=0.001,\n        fp16=False,\n        bf16=True,\n        max_grad_norm=0.3,\n        warmup_ratio=0.03,\n        group_by_length=True,\n        lr_scheduler_type=\"cosine\",\n        report_to=[\"tensorboard\"],\n        seed=42,\n    )\n\n    # Create trainer\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        tokenizer=tokenizer,\n        data_collator=transformers.DataCollatorForSeq2Seq(\n            tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n        ),\n    )\n\n    # Disable cache for gradient checkpointing\n    model.config.use_cache = False\n\n    # Train\n    print(f\"Starting training at {datetime.now()}\")\n    trainer.train()\n\n    # Save the adapter\n    model.save_pretrained(output_dir)\n    print(f\"Adapter saved to {output_dir}\")\n\n    return trainer\n\n# Usage\ntrainer = train_qlora_model(model, tokenizer, dataset, \"./my_adapter\")\n```"}
{"instruction": "Show how to load and use a trained LoRA adapter for inference", "input": "", "output": "```python\nfrom transformers import pipeline\nfrom peft import PeftModel\n\ndef load_adapter_for_inference(base_model_name: str, adapter_path: str):\n    \"\"\"Load base model with LoRA adapter for inference\"\"\"\n\n    # Load tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n\n    # Load base model\n    model = AutoModelForCausalLM.from_pretrained(\n        base_model_name,\n        torch_dtype=torch.float16,\n        device_map=\"auto\",\n        load_in_4bit=True,\n    )\n\n    # Load LoRA adapter\n    model = PeftModel.from_pretrained(model, adapter_path)\n\n    # Create pipeline\n    pipe = pipeline(\n        \"text-generation\",\n        model=model,\n        tokenizer=tokenizer,\n        torch_dtype=torch.float16,\n        device_map=\"auto\",\n    )\n\n    return pipe\n\n# Usage\ngenerator = load_adapter_for_inference(\"mistralai/Mistral-7B-v0.1\", \"./my_adapter\")\n\n# Generate text\nprompt = \"### Instruction:\\nWrite a Python function to calculate fibonacci numbers\\n\\n### Response:\\n\"\noutput = generator(\n    prompt,\n    max_new_tokens=200,\n    temperature=0.7,\n    top_p=0.95,\n    repetition_penalty=1.1\n)\nprint(output[0]['generated_text'])\n```"}
{"instruction": "Implement memory-efficient dataset loading for large files", "input": "", "output": "```python\nfrom datasets import load_dataset\nfrom itertools import islice\n\ndef load_dataset_streaming(file_path: str, tokenizer, batch_size: int = 1000):\n    \"\"\"Load large datasets in streaming mode\"\"\"\n\n    def data_generator():\n        with open(file_path, 'r') as f:\n            for line in f:\n                yield json.loads(line)\n\n    def preprocess_batch(examples):\n        # Format texts\n        texts = []\n        for ex in examples:\n            if ex.get('input'):\n                text = f\"Question: {ex['instruction']}\\nContext: {ex['input']}\\nAnswer: {ex['output']}\"\n            else:\n                text = f\"Question: {ex['instruction']}\\nAnswer: {ex['output']}\"\n            texts.append(text)\n\n        # Tokenize\n        encodings = tokenizer(\n            texts,\n            truncation=True,\n            padding=\"max_length\",\n            max_length=512,\n        )\n        encodings[\"labels\"] = encodings[\"input_ids\"].copy()\n\n        return encodings\n\n    # Process in batches\n    dataset_list = []\n    batch = []\n\n    for item in data_generator():\n        batch.append(item)\n        if len(batch) >= batch_size:\n            processed = preprocess_batch(batch)\n            dataset_list.extend([\n                {k: v[i] for k, v in processed.items()}\n                for i in range(len(batch))\n            ])\n            batch = []\n\n    # Don't forget last batch\n    if batch:\n        processed = preprocess_batch(batch)\n        dataset_list.extend([\n            {k: v[i] for k, v in processed.items()}\n            for i in range(len(batch))\n        ])\n\n    return Dataset.from_list(dataset_list)\n\n# Usage\ntrain_dataset = load_dataset_streaming(\"large_dataset.jsonl\", tokenizer)\n```"}
{"instruction": "Create a custom data collator for instruction tuning", "input": "", "output": "```python\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Any\nimport torch\n\n@dataclass\nclass InstructionDataCollator:\n    \"\"\"Custom data collator for instruction tuning\"\"\"\n    tokenizer: Any\n    max_length: int = 512\n    pad_to_multiple_of: int = 8\n    label_pad_token_id: int = -100\n\n    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n        # Extract input_ids and labels\n        input_ids = [f[\"input_ids\"] for f in features]\n        labels = [f[\"labels\"] for f in features]\n\n        # Pad inputs\n        input_ids = torch.nn.utils.rnn.pad_sequence(\n            [torch.tensor(ids) for ids in input_ids],\n            batch_first=True,\n            padding_value=self.tokenizer.pad_token_id\n        )\n\n        # Pad labels\n        labels = torch.nn.utils.rnn.pad_sequence(\n            [torch.tensor(labs) for labs in labels],\n            batch_first=True,\n            padding_value=self.label_pad_token_id\n        )\n\n        # Create attention mask\n        attention_mask = input_ids.ne(self.tokenizer.pad_token_id)\n\n        # Pad to multiple of N for efficiency\n        if self.pad_to_multiple_of:\n            remainder = input_ids.shape[1] % self.pad_to_multiple_of\n            if remainder:\n                padding = self.pad_to_multiple_of - remainder\n                input_ids = torch.nn.functional.pad(input_ids, (0, padding), value=self.tokenizer.pad_token_id)\n                labels = torch.nn.functional.pad(labels, (0, padding), value=self.label_pad_token_id)\n                attention_mask = torch.nn.functional.pad(attention_mask, (0, padding), value=0)\n\n        return {\n            \"input_ids\": input_ids,\n            \"labels\": labels,\n            \"attention_mask\": attention_mask,\n        }\n\n# Usage\ndata_collator = InstructionDataCollator(tokenizer=tokenizer)\n```"}
{"instruction": "Show how to merge LoRA adapters with the base model", "input": "", "output": "```python\ndef merge_adapter_with_base_model(base_model_name: str, adapter_path: str, output_path: str):\n    \"\"\"Merge LoRA adapter with base model and save\"\"\"\n\n    print(\"Loading base model...\")\n    model = AutoModelForCausalLM.from_pretrained(\n        base_model_name,\n        torch_dtype=torch.float16,\n        device_map=\"auto\",\n    )\n\n    print(\"Loading adapter...\")\n    model = PeftModel.from_pretrained(model, adapter_path)\n\n    print(\"Merging adapter with base model...\")\n    model = model.merge_and_unload()\n\n    print(f\"Saving merged model to {output_path}\")\n    model.save_pretrained(output_path)\n\n    # Also save tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n    tokenizer.save_pretrained(output_path)\n\n    print(\"Merge complete!\")\n    return model\n\n# Usage\nmerged_model = merge_adapter_with_base_model(\n    \"mistralai/Mistral-7B-v0.1\",\n    \"./my_adapter\",\n    \"./merged_model\"\n)\n```"}
{"instruction": "Set up distributed training across multiple GPUs", "input": "", "output": "```python\nimport torch.distributed as dist\nfrom accelerate import Accelerator\n\ndef setup_distributed_training():\n    \"\"\"Setup for multi-GPU training\"\"\"\n\n    # Initialize accelerator\n    accelerator = Accelerator(\n        gradient_accumulation_steps=4,\n        mixed_precision=\"bf16\",\n        log_with=\"tensorboard\",\n        project_dir=\"./logs\"\n    )\n\n    # Model setup with device map\n    if accelerator.num_processes > 1:\n        device_map = {\"\": accelerator.device}\n    else:\n        device_map = \"auto\"\n\n    # Load model\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        quantization_config=bnb_config,\n        device_map=device_map,\n        torch_dtype=torch.bfloat16,\n    )\n\n    # Training arguments for distributed\n    training_args = TrainingArguments(\n        output_dir=\"./output\",\n        per_device_train_batch_size=4,\n        gradient_accumulation_steps=4,\n        num_train_epochs=3,\n        learning_rate=2e-4,\n        bf16=True,\n        logging_steps=10,\n        save_steps=500,\n        ddp_find_unused_parameters=False,\n        gradient_checkpointing=True,\n        deepspeed=\"deepspeed_config.json\" if accelerator.num_processes > 1 else None,\n    )\n\n    return accelerator, model, training_args\n\n# DeepSpeed config (save as deepspeed_config.json)\ndeepspeed_config = {\n    \"bf16\": {\n        \"enabled\": \"auto\"\n    },\n    \"zero_optimization\": {\n        \"stage\": 2,\n        \"offload_optimizer\": {\n            \"device\": \"cpu\",\n            \"pin_memory\": True\n        },\n        \"allgather_partitions\": True,\n        \"allgather_bucket_size\": 2e8,\n        \"overlap_comm\": True,\n        \"reduce_scatter\": True,\n        \"reduce_bucket_size\": 2e8,\n        \"contiguous_gradients\": True\n    },\n    \"gradient_accumulation_steps\": \"auto\",\n    \"gradient_clipping\": \"auto\",\n    \"train_batch_size\": \"auto\",\n    \"train_micro_batch_size_per_gpu\": \"auto\"\n}\n```"}
{"instruction": "Implement evaluation metrics for fine-tuned models", "input": "", "output": "```python\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\nfrom tqdm import tqdm\n\ndef evaluate_model(model, tokenizer, eval_dataset, max_samples=100):\n    \"\"\"Evaluate model performance on a dataset\"\"\"\n\n    model.eval()\n    predictions = []\n    references = []\n\n    # Sample subset for evaluation\n    eval_samples = random.sample(range(len(eval_dataset)), min(max_samples, len(eval_dataset)))\n\n    with torch.no_grad():\n        for idx in tqdm(eval_samples, desc=\"Evaluating\"):\n            sample = eval_dataset[idx]\n\n            # Get input text (without the answer)\n            input_text = tokenizer.decode(sample['input_ids'], skip_special_tokens=True)\n            # Find where the response starts\n            response_start = input_text.find(\"### Response:\")\n            if response_start == -1:\n                continue\n\n            prompt = input_text[:response_start + len(\"### Response:\\n\")]\n            true_response = input_text[response_start + len(\"### Response:\\n\"):]\n\n            # Generate prediction\n            inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n            outputs = model.generate(\n                **inputs,\n                max_new_tokens=len(tokenizer.encode(true_response)) + 50,\n                temperature=0.1,\n                do_sample=True,\n                top_p=0.95,\n            )\n\n            generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n            generated_response = generated[len(prompt):].strip()\n\n            predictions.append(generated_response)\n            references.append(true_response)\n\n    # Calculate metrics\n    from rouge import Rouge\n    rouge = Rouge()\n\n    scores = rouge.get_scores(predictions, references, avg=True)\n\n    return {\n        \"rouge-1\": scores[\"rouge-1\"][\"f\"],\n        \"rouge-2\": scores[\"rouge-2\"][\"f\"],\n        \"rouge-l\": scores[\"rouge-l\"][\"f\"],\n        \"samples_evaluated\": len(predictions)\n    }\n\n# Usage\nmetrics = evaluate_model(model, tokenizer, eval_dataset)\nprint(f\"Evaluation results: {metrics}\")\n```"}
{"instruction": "Create a checkpoint manager for training", "input": "", "output": "```python\nimport os\nimport shutil\nfrom pathlib import Path\n\nclass CheckpointManager:\n    \"\"\"Manage model checkpoints during training\"\"\"\n\n    def __init__(self, output_dir: str, max_checkpoints: int = 3):\n        self.output_dir = Path(output_dir)\n        self.max_checkpoints = max_checkpoints\n        self.output_dir.mkdir(parents=True, exist_ok=True)\n\n    def save_checkpoint(self, model, tokenizer, epoch: int, step: int, metrics: dict = None):\n        \"\"\"Save a checkpoint\"\"\"\n        checkpoint_name = f\"checkpoint-epoch{epoch}-step{step}\"\n        checkpoint_path = self.output_dir / checkpoint_name\n\n        # Save model and tokenizer\n        model.save_pretrained(checkpoint_path)\n        tokenizer.save_pretrained(checkpoint_path)\n\n        # Save metrics if provided\n        if metrics:\n            with open(checkpoint_path / \"metrics.json\", \"w\") as f:\n                json.dump(metrics, f, indent=2)\n\n        # Save training state\n        state = {\n            \"epoch\": epoch,\n            \"step\": step,\n            \"timestamp\": datetime.now().isoformat()\n        }\n        with open(checkpoint_path / \"training_state.json\", \"w\") as f:\n            json.dump(state, f, indent=2)\n\n        print(f\"Checkpoint saved: {checkpoint_name}\")\n\n        # Clean old checkpoints\n        self._cleanup_old_checkpoints()\n\n    def _cleanup_old_checkpoints(self):\n        \"\"\"Remove old checkpoints, keeping only the most recent ones\"\"\"\n        checkpoints = sorted(\n            [d for d in self.output_dir.iterdir() if d.is_dir() and d.name.startswith(\"checkpoint-\")],\n            key=lambda x: x.stat().st_mtime\n        )\n\n        while len(checkpoints) > self.max_checkpoints:\n            oldest = checkpoints.pop(0)\n            shutil.rmtree(oldest)\n            print(f\"Removed old checkpoint: {oldest.name}\")\n\n    def get_latest_checkpoint(self):\n        \"\"\"Get the path to the latest checkpoint\"\"\"\n        checkpoints = [d for d in self.output_dir.iterdir() if d.is_dir() and d.name.startswith(\"checkpoint-\")]\n        if not checkpoints:\n            return None\n        return max(checkpoints, key=lambda x: x.stat().st_mtime)\n\n# Usage\ncheckpoint_manager = CheckpointManager(\"./checkpoints\")\ncheckpoint_manager.save_checkpoint(model, tokenizer, epoch=1, step=100, metrics={\"loss\": 0.5})\n```"}
{"instruction": "Show how to mix multiple datasets with different sampling weights", "input": "", "output": "```python\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\n\nclass MixedDataset(Dataset):\n    \"\"\"Mix multiple datasets with weighted sampling\"\"\"\n\n    def __init__(self, datasets: List[Dataset], weights: List[float]):\n        self.datasets = datasets\n        self.weights = np.array(weights) / np.sum(weights)  # Normalize\n        self.cumulative_sizes = np.cumsum([len(d) for d in datasets])\n        self.total_size = sum(len(d) for d in datasets)\n\n    def __len__(self):\n        return self.total_size\n\n    def __getitem__(self, idx):\n        # Randomly select dataset based on weights\n        dataset_idx = np.random.choice(len(self.datasets), p=self.weights)\n        dataset = self.datasets[dataset_idx]\n\n        # Get random item from selected dataset\n        item_idx = np.random.randint(0, len(dataset))\n        return dataset[item_idx]\n\n# Example usage\ndef create_mixed_training_data():\n    # Load different datasets\n    code_dataset = load_dataset_streaming(\"python_code.jsonl\", tokenizer)\n    instruction_dataset = load_dataset_streaming(\"instructions.jsonl\", tokenizer)\n    qa_dataset = load_dataset_streaming(\"qa_pairs.jsonl\", tokenizer)\n\n    # Mix with weights (more weight on code)\n    mixed_dataset = MixedDataset(\n        datasets=[code_dataset, instruction_dataset, qa_dataset],\n        weights=[0.5, 0.3, 0.2]  # 50% code, 30% instructions, 20% Q&A\n    )\n\n    # Create dataloader\n    dataloader = DataLoader(\n        mixed_dataset,\n        batch_size=4,\n        shuffle=True,\n        collate_fn=InstructionDataCollator(tokenizer)\n    )\n\n    return dataloader\n\n# Usage\ntrain_dataloader = create_mixed_training_data()\n```"}
{"instruction": "Integrate a RAG system with a fine-tuned model", "input": "", "output": "```python\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.vectorstores import FAISS\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\nclass RAGPipeline:\n    \"\"\"RAG pipeline with custom fine-tuned model\"\"\"\n\n    def __init__(self, model, tokenizer, embedding_model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)\n        self.vector_store = None\n\n    def index_documents(self, documents: List[str]):\n        \"\"\"Index documents for retrieval\"\"\"\n        # Split documents\n        text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=500,\n            chunk_overlap=50,\n            length_function=len,\n        )\n\n        chunks = []\n        for doc in documents:\n            chunks.extend(text_splitter.split_text(doc))\n\n        # Create vector store\n        self.vector_store = FAISS.from_texts(chunks, self.embeddings)\n        print(f\"Indexed {len(chunks)} chunks\")\n\n    def query(self, question: str, k: int = 3):\n        \"\"\"Query with retrieval-augmented generation\"\"\"\n        if not self.vector_store:\n            return \"No documents indexed yet.\"\n\n        # Retrieve relevant chunks\n        docs = self.vector_store.similarity_search(question, k=k)\n        context = \"\\n\\n\".join([doc.page_content for doc in docs])\n\n        # Create prompt with context\n        prompt = f\"\"\"Context: {context}\n\nQuestion: {question}\n\nBased on the context above, please provide a detailed answer:\n\"\"\"\n\n        # Generate response\n        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n\n        with torch.no_grad():\n            outputs = self.model.generate(\n                **inputs,\n                max_new_tokens=256,\n                temperature=0.7,\n                do_sample=True,\n                top_p=0.95,\n            )\n\n        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n        # Extract only the answer part\n        answer = response[len(prompt):].strip()\n\n        return answer\n\n# Usage\nrag = RAGPipeline(model, tokenizer)\nrag.index_documents([\"Document 1 content...\", \"Document 2 content...\"])\nanswer = rag.query(\"What is mentioned in the documents about X?\")\n```"}
{"instruction": "Implement comprehensive training monitoring and logging", "input": "", "output": "```python\nimport wandb\nfrom torch.utils.tensorboard import SummaryWriter\nimport matplotlib.pyplot as plt\n\nclass TrainingMonitor:\n    \"\"\"Monitor and log training progress\"\"\"\n\n    def __init__(self, project_name: str, run_name: str, use_wandb: bool = True):\n        self.use_wandb = use_wandb\n        self.step = 0\n\n        # Initialize logging\n        if use_wandb:\n            wandb.init(project=project_name, name=run_name)\n\n        self.writer = SummaryWriter(f\"runs/{run_name}\")\n        self.metrics_history = {\n            \"loss\": [],\n            \"learning_rate\": [],\n            \"grad_norm\": []\n        }\n\n    def log_metrics(self, metrics: dict, step: int = None):\n        \"\"\"Log metrics to all trackers\"\"\"\n        if step is None:\n            step = self.step\n\n        # Log to wandb\n        if self.use_wandb:\n            wandb.log(metrics, step=step)\n\n        # Log to tensorboard\n        for key, value in metrics.items():\n            self.writer.add_scalar(key, value, step)\n\n        # Store in history\n        for key, value in metrics.items():\n            if key in self.metrics_history:\n                self.metrics_history[key].append(value)\n\n        self.step = step + 1\n\n    def log_model_predictions(self, prompts: List[str], predictions: List[str], step: int):\n        \"\"\"Log example predictions\"\"\"\n        table_data = []\n        for prompt, pred in zip(prompts, predictions):\n            table_data.append([prompt, pred])\n\n        if self.use_wandb:\n            table = wandb.Table(columns=[\"Prompt\", \"Prediction\"], data=table_data)\n            wandb.log({\"predictions\": table}, step=step)\n\n    def plot_training_curves(self):\n        \"\"\"Plot and save training curves\"\"\"\n        fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n\n        # Plot loss\n        if self.metrics_history[\"loss\"]:\n            axes[0, 0].plot(self.metrics_history[\"loss\"])\n            axes[0, 0].set_title(\"Training Loss\")\n            axes[0, 0].set_xlabel(\"Step\")\n            axes[0, 0].set_ylabel(\"Loss\")\n\n        # Plot learning rate\n        if self.metrics_history[\"learning_rate\"]:\n            axes[0, 1].plot(self.metrics_history[\"learning_rate\"])\n            axes[0, 1].set_title(\"Learning Rate\")\n            axes[0, 1].set_xlabel(\"Step\")\n            axes[0, 1].set_ylabel(\"LR\")\n\n        # Plot gradient norm\n        if self.metrics_history[\"grad_norm\"]:\n            axes[1, 0].plot(self.metrics_history[\"grad_norm\"])\n            axes[1, 0].set_title(\"Gradient Norm\")\n            axes[1, 0].set_xlabel(\"Step\")\n            axes[1, 0].set_ylabel(\"Norm\")\n\n        plt.tight_layout()\n        plt.savefig(\"training_curves.png\")\n\n        if self.use_wandb:\n            wandb.log({\"training_curves\": wandb.Image(\"training_curves.png\")})\n\n    def finish(self):\n        \"\"\"Clean up logging\"\"\"\n        self.plot_training_curves()\n        self.writer.close()\n        if self.use_wandb:\n            wandb.finish()\n\n# Usage in training loop\nmonitor = TrainingMonitor(\"llm-finetuning\", \"mistral-qlora-run1\")\n\nfor step, batch in enumerate(train_dataloader):\n    outputs = model(**batch)\n    loss = outputs.loss\n\n    # Log metrics\n    monitor.log_metrics({\n        \"loss\": loss.item(),\n        \"learning_rate\": optimizer.param_groups[0][\"lr\"],\n        \"grad_norm\": torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n    }, step=step)\n\n    # Log predictions every 100 steps\n    if step % 100 == 0:\n        test_prompts = [\"Write a function to\", \"Explain how to\"]\n        with torch.no_grad():\n            predictions = [generate_text(model, tokenizer, p) for p in test_prompts]\n        monitor.log_model_predictions(test_prompts, predictions, step)\n\nmonitor.finish()\n```"}
