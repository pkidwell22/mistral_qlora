{"instruction": "Implement evaluation metrics for fine-tuned models", "input": "", "output": "```python\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\nfrom tqdm import tqdm\n\ndef evaluate_model(model, tokenizer, eval_dataset, max_samples=100):\n    \"\"\"Evaluate model performance on a dataset\"\"\"\n\n    model.eval()\n    predictions = []\n    references = []\n\n    # Sample subset for evaluation\n    eval_samples = random.sample(range(len(eval_dataset)), min(max_samples, len(eval_dataset)))\n\n    with torch.no_grad():\n        for idx in tqdm(eval_samples, desc=\"Evaluating\"):\n            sample = eval_dataset[idx]\n\n            # Get input text (without the answer)\n            input_text = tokenizer.decode(sample['input_ids'], skip_special_tokens=True)\n            # Find where the response starts\n            response_start = input_text.find(\"### Response:\")\n            if response_start == -1:\n                continue\n\n            prompt = input_text[:response_start + len(\"### Response:\\n\")]\n            true_response = input_text[response_start + len(\"### Response:\\n\"):]\n\n            # Generate prediction\n            inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n            outputs = model.generate(\n                **inputs,\n                max_new_tokens=len(tokenizer.encode(true_response)) + 50,\n                temperature=0.1,\n                do_sample=True,\n                top_p=0.95,\n            )\n\n            generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n            generated_response = generated[len(prompt):].strip()\n\n            predictions.append(generated_response)\n            references.append(true_response)\n\n    # Calculate metrics\n    from rouge import Rouge\n    rouge = Rouge()\n\n    scores = rouge.get_scores(predictions, references, avg=True)\n\n    return {\n        \"rouge-1\": scores[\"rouge-1\"][\"f\"],\n        \"rouge-2\": scores[\"rouge-2\"][\"f\"],\n        \"rouge-l\": scores[\"rouge-l\"][\"f\"],\n        \"samples_evaluated\": len(predictions)\n    }\n\n# Usage\nmetrics = evaluate_model(model, tokenizer, eval_dataset)\nprint(f\"Evaluation results: {metrics}\")\n```"}
