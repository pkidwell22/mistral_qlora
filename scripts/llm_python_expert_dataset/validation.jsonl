{"instruction":"Optimize model inference for production deployment","input":"","output":"```python\nimport torch\nfrom transformers import TextStreamer\n\nclass OptimizedInference:\n    \"\"\"Optimized inference pipeline for production\"\"\"\n\n    def __init__(self, model, tokenizer):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.device = next(model.parameters()).device\n\n        # Enable eval mode\n        self.model.eval()\n\n        # Compile model for faster inference (PyTorch 2.0+)\n        if hasattr(torch, 'compile'):\n            self.model = torch.compile(model)\n\n    @torch.inference_mode()\n    def generate(self, prompt: str, max_new_tokens: int = 256, stream: bool = False):\n        \"\"\"Generate text with optimizations\"\"\"\n\n        # Tokenize\n        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n\n        # Streaming\n        if stream:\n            streamer = TextStreamer(self.tokenizer, skip_special_tokens=True)\n            generation_kwargs = dict(\n                **inputs,\n                max_new_tokens=max_new_tokens,\n                temperature=0.7,\n                do_sample=True,\n                top_p=0.95,\n                streamer=streamer,\n            )\n        else:\n            generation_kwargs = dict(\n                **inputs,\n                max_new_tokens=max_new_tokens,\n                temperature=0.7,\n                do_sample=True,\n                top_p=0.95,\n            )\n\n        # Generate\n        outputs = self.model.generate(**generation_kwargs)\n\n        if not stream:\n            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n            # Remove prompt from response\n            response = response[len(prompt):].strip()\n            return response\n\n    def batch_generate(self, prompts: List[str], max_new_tokens: int = 256):\n        \"\"\"Batch generation for efficiency\"\"\"\n\n        # Tokenize all prompts\n        inputs = self.tokenizer(\n            prompts,\n            padding=True,\n            truncation=True,\n            return_tensors=\"pt\"\n        ).to(self.device)\n\n        # Generate\n        with torch.cuda.amp.autocast():  # Mixed precision\n            outputs = self.model.generate(\n                **inputs,\n                max_new_tokens=max_new_tokens,\n                temperature=0.7,\n                do_sample=True,\n                top_p=0.95,\n                pad_token_id=self.tokenizer.pad_token_id,\n            )\n\n        # Decode\n        responses = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\n        # Remove prompts\n        cleaned_responses = []\n        for prompt, response in zip(prompts, responses):\n            cleaned = response[len(prompt):].strip()\n            cleaned_responses.append(cleaned)\n\n        return cleaned_responses\n\n# Usage\ninference = OptimizedInference(model, tokenizer)\nresponse = inference.generate(\"Write a Python function to sort a list\")\n```","text":"### Instruction:\nOptimize model inference for production deployment\n\n### Response:\n```python\nimport torch\nfrom transformers import TextStreamer\n\nclass OptimizedInference:\n    \"\"\"Optimized inference pipeline for production\"\"\"\n\n    def __init__(self, model, tokenizer):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.device = next(model.parameters()).device\n\n        # Enable eval mode\n        self.model.eval()\n\n        # Compile model for faster inference (PyTorch 2.0+)\n        if hasattr(torch, 'compile'):\n            self.model = torch.compile(model)\n\n    @torch.inference_mode()\n    def generate(self, prompt: str, max_new_tokens: int = 256, stream: bool = False):\n        \"\"\"Generate text with optimizations\"\"\"\n\n        # Tokenize\n        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n\n        # Streaming\n        if stream:\n            streamer = TextStreamer(self.tokenizer, skip_special_tokens=True)\n            generation_kwargs = dict(\n                **inputs,\n                max_new_tokens=max_new_tokens,\n                temperature=0.7,\n                do_sample=True,\n                top_p=0.95,\n                streamer=streamer,\n            )\n        else:\n            generation_kwargs = dict(\n                **inputs,\n                max_new_tokens=max_new_tokens,\n                temperature=0.7,\n                do_sample=True,\n                top_p=0.95,\n            )\n\n        # Generate\n        outputs = self.model.generate(**generation_kwargs)\n\n        if not stream:\n            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n            # Remove prompt from response\n            response = response[len(prompt):].strip()\n            return response\n\n    def batch_generate(self, prompts: List[str], max_new_tokens: int = 256):\n        \"\"\"Batch generation for efficiency\"\"\"\n\n        # Tokenize all prompts\n        inputs = self.tokenizer(\n            prompts,\n            padding=True,\n            truncation=True,\n            return_tensors=\"pt\"\n        ).to(self.device)\n\n        # Generate\n        with torch.cuda.amp.autocast():  # Mixed precision\n            outputs = self.model.generate(\n                **inputs,\n                max_new_tokens=max_new_tokens,\n                temperature=0.7,\n                do_sample=True,\n                top_p=0.95,\n                pad_token_id=self.tokenizer.pad_token_id,\n            )\n\n        # Decode\n        responses = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\n        # Remove prompts\n        cleaned_responses = []\n        for prompt, response in zip(prompts, responses):\n            cleaned = response[len(prompt):].strip()\n            cleaned_responses.append(cleaned)\n\n        return cleaned_responses\n\n# Usage\ninference = OptimizedInference(model, tokenizer)\nresponse = inference.generate(\"Write a Python function to sort a list\")\n```"}
