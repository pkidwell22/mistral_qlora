# chunk_gutenberg_phase2_streamed.py

from datasets import load_from_disk, Dataset, concatenate_datasets
from pathlib import Path
import time

# â”€â”€â”€ Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
INTERMEDIATE_PATH = Path(r"C:/Users/pkidw/PycharmProjects/mistral_qlora/local_datasets/temp_chunked_gutenberg")
FINAL_OUTPUT_PATH = Path(r"C:/Users/pkidw/PycharmProjects/mistral_qlora/local_datasets/project_gutenberg_chunks")
BATCH_SIZE = 10_000   # Number of input_ids per saved file
MERGE_FINAL = False   # Set to True to merge parts after chunking

# â”€â”€â”€ Setup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
start_time = time.time()
output_dir = FINAL_OUTPUT_PATH
output_dir.mkdir(parents=True, exist_ok=True)

# â”€â”€â”€ Load Dataset â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print(f"ðŸ“‚ Loading dataset from: {INTERMEDIATE_PATH}")
dataset = load_from_disk(str(INTERMEDIATE_PATH))

print(f"ðŸ“¦ Flattening 'input_ids' in batches of {BATCH_SIZE}...")

# â”€â”€â”€ Streaming Flatten + Save â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
flat_input_ids = []
total_written = 0
part = 0

for i, example in enumerate(dataset):
    flat_input_ids.extend(example["input_ids"])

    if len(flat_input_ids) >= BATCH_SIZE:
        part_dataset = Dataset.from_dict({"input_ids": flat_input_ids})
        part_path = output_dir / f"part_{part}"
        part_dataset.save_to_disk(str(part_path))
        print(f"ðŸ’¾ Saved {len(flat_input_ids):,} input_ids â†’ {part_path.name}")
        total_written += len(flat_input_ids)
        flat_input_ids = []
        part += 1

# â”€â”€â”€ Final Flush â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if flat_input_ids:
    part_dataset = Dataset.from_dict({"input_ids": flat_input_ids})
    part_path = output_dir / f"part_{part}"
    part_dataset.save_to_disk(str(part_path))
    print(f"ðŸ’¾ Saved final {len(flat_input_ids):,} input_ids â†’ {part_path.name}")
    total_written += len(flat_input_ids)

print(f"âœ… Finished saving {total_written:,} input_ids across {part + 1} parts in {time.time() - start_time:.2f}s.")

# â”€â”€â”€ Optional: Merge All â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if MERGE_FINAL:
    print("ðŸ§ª Merging parts into single dataset...")
    parts = [load_from_disk(str(output_dir / f"part_{i}")) for i in range(part + 1)]
    merged = concatenate_datasets(parts)
    merged_path = output_dir / "merged"
    merged.save_to_disk(str(merged_path))
    print(f"ðŸŽ‰ Merged dataset saved to: {merged_path}")
